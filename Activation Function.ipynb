{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Activation Functions Notebook\n",
    "\n",
    "# Question 1: What is an activation function in the context of artificial neural networks?\n",
    "\"\"\"\n",
    "An activation function determines the output of a neural network node given its inputs. It introduces non-linearity into the network, allowing it to learn complex patterns. Without activation functions, neural networks would just be linear regression models.\n",
    "\"\"\"\n",
    "\n",
    "# Question 2: What are some common types of activation functions used in neural networks?\n",
    "\"\"\"\n",
    "Common activation functions include:\n",
    "1. Sigmoid (Logistic)\n",
    "2. Tanh (Hyperbolic Tangent)\n",
    "3. ReLU (Rectified Linear Unit)\n",
    "4. Leaky ReLU\n",
    "5. Softmax (for output layers in classification)\n",
    "6. Swish\n",
    "7. ELU (Exponential Linear Unit)\n",
    "\"\"\"\n",
    "\n",
    "# Question 3: How do activation functions affect the training process and performance of a neural network?\n",
    "\"\"\"\n",
    "Activation functions affect:\n",
    "1. Gradient flow during backpropagation\n",
    "2. Training speed/convergence\n",
    "3. Model accuracy\n",
    "4. Susceptibility to vanishing/exploding gradients\n",
    "5. Network's ability to learn complex patterns\n",
    "6. Computational efficiency\n",
    "\"\"\"\n",
    "\n",
    "# Question 4: How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\"\"\"\n",
    "Sigmoid function: Ïƒ(x) = 1/(1 + e^-x)\n",
    "\n",
    "Advantages:\n",
    "1. Smooth gradient\n",
    "2. Outputs between 0-1 (good for probabilities)\n",
    "3. Easy derivatives\n",
    "\n",
    "Disadvantages:\n",
    "1. Suffers from vanishing gradients\n",
    "2. Not zero-centered\n",
    "3. Computationally expensive\n",
    "4. Outputs not symmetric around zero\n",
    "\"\"\"\n",
    "\n",
    "# Question 5: What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\"\"\"\n",
    "ReLU function: f(x) = max(0, x)\n",
    "\n",
    "Differences from sigmoid:\n",
    "1. Computationally simpler\n",
    "2. Avoids vanishing gradient problem\n",
    "3. Outputs are not bounded\n",
    "4. Not smooth (has a kink at 0)\n",
    "5. Can cause \"dying ReLU\" problem\n",
    "6. More efficient for deep networks\n",
    "7. Typically leads to faster convergence\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
