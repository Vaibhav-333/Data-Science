
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 1 - What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 1** -\n",
    "Ridge Regression is a type of linear regression that includes a regularization term (L2 penalty). While ordinary least squares (OLS) minimizes the sum of squared residuals, Ridge Regression minimizes the sum of squared residuals plus a penalty term proportional to the square of the magnitude of the coefficients.\n",
    "\n",
    "Mathematically:\n",
    "OLS objective: \( \min_{\beta} \sum_{i=1}^n (y_i - X_i\beta)^2 \)\n",
    "Ridge objective: \( \min_{\beta} \sum_{i=1}^n (y_i - X_i\beta)^2 + \lambda \sum_{j=1}^p \beta_j^2 \)\n",
    "Here, \( \lambda \) is the regularization parameter. Ridge regression shrinks the coefficients and is especially useful when multicollinearity is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 2 - What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 2** -\n",
    "Ridge Regression shares most assumptions with ordinary least squares regression:\n",
    "1. **Linearity**: The relationship between predictors and outcome is linear.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "3. **Homoscedasticity**: Constant variance of error terms.\n",
    "4. **Normality**: Errors are normally distributed (important for inference).\n",
    "However, Ridge Regression **relaxes** the assumption of no multicollinearity by adding regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 3 - How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 3** -\n",
    "The tuning parameter \( \lambda \) is typically selected using **cross-validation**, such as k-fold cross-validation.\n",
    "\n",
    "Steps:\n",
    "1. Define a grid of possible \( \lambda \) values.\n",
    "2. Train the Ridge Regression model on training folds for each \( \lambda \).\n",
    "3. Evaluate performance on validation fold.\n",
    "4. Choose \( \lambda \) that minimizes validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 4 - Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 4** -\n",
    "Ridge Regression **does not** perform feature selection in the strict sense. It **shrinks** coefficients but does not set them exactly to zero. Thus, all features remain in the model.\n",
    "\n",
    "For actual feature selection, **Lasso Regression** (L1 penalty) is preferred, as it can zero out coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 5 - How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 5** -\n",
    "Ridge Regression performs well in the presence of **multicollinearity**.\n",
    "\n",
    "Multicollinearity inflates variance of OLS estimates. Ridge reduces this by adding bias through regularization, leading to more stable and reliable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 6 - Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 6** -\n",
    "Yes, Ridge Regression can handle both **categorical** and **continuous** variables.\n",
    "Categorical variables need to be **encoded** (e.g., one-hot encoding) before applying Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 7 - How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 7** -\n",
    "Coefficients in Ridge Regression represent the **effect** of each feature on the outcome, **controlling for other variables**, but they are **shrunk toward zero** compared to OLS.\n",
    "\n",
    "Their exact values depend on the value of \( \lambda \). Larger \( \lambda \) results in smaller coefficients. Interpretation is still in terms of unit change, but they are biased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question no. 8 - Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer no. 8** -\n",
    "Yes, Ridge Regression can be applied to **time-series data**, but with caution.\n",
    "\n",
    "Key considerations:\n",
    "- Use **lagged variables** as features.\n",
    "- Ensure **stationarity** or apply differencing.\n",
    "- Cross-validation should respect temporal order (e.g., **time-series split**).\n",
    "Ridge helps when features (e.g., lags) are highly correlated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
