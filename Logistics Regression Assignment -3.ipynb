{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c94edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model Evaluation Metrics Notebook\n",
    "\n",
    "# Question 1: Precision and Recall\n",
    "\"\"\"\n",
    "Precision:\n",
    "- Measures model's accuracy for positive predictions\n",
    "- Formula: TP / (TP + FP)\n",
    "- Focus: \"How many selected items are relevant?\"\n",
    "- Important when FP are costly (e.g., spam detection)\n",
    "\n",
    "Recall:\n",
    "- Measures model's ability to find all positives\n",
    "- Formula: TP / (TP + FN)\n",
    "- Focus: \"How many relevant items are selected?\"\n",
    "- Important when FN are costly (e.g., cancer diagnosis)\n",
    "\n",
    "Example:\n",
    "For email spam detection:\n",
    "- High precision: Few legitimate emails marked as spam\n",
    "- High recall: Few spam emails missed in detection\n",
    "\"\"\"\n",
    "\n",
    "# Question 2: F1 Score\n",
    "\"\"\"\n",
    "F1 Score:\n",
    "- Harmonic mean of precision and recall\n",
    "- Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- Range: 0 (worst) to 1 (best)\n",
    "\n",
    "Key Differences:\n",
    "1. Balances precision and recall\n",
    "2. Better metric for imbalanced datasets\n",
    "3. More conservative than accuracy\n",
    "4. Useful when need single metric for comparison\n",
    "\n",
    "When to use:\n",
    "- Class imbalance exists\n",
    "- Both FP and FN are important\n",
    "- Need single performance metric\n",
    "\"\"\"\n",
    "\n",
    "# Question 3: ROC and AUC\n",
    "\"\"\"\n",
    "ROC Curve (Receiver Operating Characteristic):\n",
    "- Plots True Positive Rate (Recall) vs False Positive Rate (FPR)\n",
    "- FPR = FP / (FP + TN)\n",
    "- Shows trade-off between sensitivity and specificity\n",
    "- Generated by varying classification threshold\n",
    "\n",
    "AUC (Area Under Curve):\n",
    "- Measures entire area under ROC curve\n",
    "- Range: 0.5 (random) to 1 (perfect)\n",
    "- Interpretation:\n",
    "  - 0.9-1.0 = Excellent\n",
    "  - 0.8-0.9 = Good\n",
    "  - 0.7-0.8 = Fair\n",
    "  - 0.5-0.7 = Poor\n",
    "\n",
    "Usage:\n",
    "- Compare models visually\n",
    "- Evaluate threshold-independent performance\n",
    "- Assess model across all classification thresholds\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
