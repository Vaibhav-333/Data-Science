{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a0e8bc",
   "metadata": {},
   "source": [
    "### Question 1: Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d3021",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "A1. R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It is calculated as:\n",
    "\n",
    "R² = 1 - (SS_res / SS_tot),\n",
    "\n",
    "where SS_res is the residual sum of squares and SS_tot is the total sum of squares. A value closer to 1 indicates that the model explains a large portion of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d6316",
   "metadata": {},
   "source": [
    "### Question 2: Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da905078",
   "metadata": {},
   "source": [
    "**Answer 2:**\n",
    "\n",
    "A2. Adjusted R-squared adjusts the R² value for the number of predictors in the model. It penalizes the addition of irrelevant predictors. The formula is:\n",
    "\n",
    "Adjusted R² = 1 - [(1 - R²)(n - 1)/(n - k - 1)]\n",
    "\n",
    "where n is the number of observations and k is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520b6e7",
   "metadata": {},
   "source": [
    "### Question 3: Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ac621",
   "metadata": {},
   "source": [
    "**Answer 3:**\n",
    "\n",
    "A3. Adjusted R-squared is more appropriate when comparing models with a different number of predictors, as it accounts for the diminishing returns of adding more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07663ba5",
   "metadata": {},
   "source": [
    "### Question 4: Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea9832",
   "metadata": {},
   "source": [
    "**Answer 4:**\n",
    "\n",
    "A4. RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are performance metrics:\n",
    "- MAE = mean(|y_true - y_pred|)\n",
    "- MSE = mean((y_true - y_pred)^2)\n",
    "- RMSE = sqrt(MSE)\n",
    "\n",
    "They represent average error magnitudes; MSE and RMSE penalize large errors more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2ae6c",
   "metadata": {},
   "source": [
    "### Question 5: Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc60f4",
   "metadata": {},
   "source": [
    "**Answer 5:**\n",
    "\n",
    "A5. MAE is robust to outliers but may under-penalize large errors. MSE and RMSE give higher weight to large errors, making them sensitive to outliers. RMSE is more interpretable in original units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073782de",
   "metadata": {},
   "source": [
    "### Question 6: Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2fbd",
   "metadata": {},
   "source": [
    "**Answer 6:**\n",
    "\n",
    "A6. Lasso regularization adds an L1 penalty to the loss function, encouraging sparsity by shrinking some coefficients to zero. Ridge adds an L2 penalty, shrinking coefficients but not eliminating them. Use Lasso when feature selection is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93068213",
   "metadata": {},
   "source": [
    "### Question 7: Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0f423",
   "metadata": {},
   "source": [
    "**Answer 7:**\n",
    "\n",
    "A7. Regularized models constrain model complexity, reducing overfitting by penalizing large coefficients. For example, Ridge regression on a high-dimensional dataset reduces model variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5581f06",
   "metadata": {},
   "source": [
    "### Question 8: Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b5f12",
   "metadata": {},
   "source": [
    "**Answer 8:**\n",
    "\n",
    "A8. Limitations include assumptions of linearity, sensitivity to feature scaling, and inability to capture complex nonlinear relationships. In such cases, tree-based models may perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2b12b",
   "metadata": {},
   "source": [
    "### Question 9: Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85268c",
   "metadata": {},
   "source": [
    "**Answer 9:**\n",
    "\n",
    "A9. MAE of 8 suggests smaller average errors, but RMSE of 10 may indicate better performance on most values with some large outliers. The choice depends on the context and tolerance for large errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29efa9",
   "metadata": {},
   "source": [
    "### Question 10: Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0adec3",
   "metadata": {},
   "source": [
    "**Answer 10:**\n",
    "\n",
    "A10. If Model B with Lasso performs similarly or better, it may be preferable due to feature selection benefits. However, higher λ can oversimplify the model. Ridge may retain all features but risks multicollinearity."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}